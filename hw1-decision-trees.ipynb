{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student #1 ID: 207047150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student #2 ID: 206727638"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bd0516e7cb654f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Decision Trees\n",
    "\n",
    "In this assignment you will implement a Decision Tree algorithm as learned in class.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "1. Write **efficient vectorized** code whenever possible. Some calculations in this exercise take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deduction.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit. Those tests will not be graded nor checked.\n",
    "1. You are free to add code and markdown cells as you see fit.\n",
    "1. Write your functions in this jupyter notebook only. Do not create external python modules and import from them.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only, unless otherwise mentioned.\n",
    "1. Your code must run without errors. It is a good idea to restart the notebook and run it from end to end before you submit your exercise.\n",
    "1. Answers to qualitative questions should be written in **markdown cells (with $\\LaTeX$ support)**.\n",
    "1. Submit this jupyter notebook only using your ID as a filename. **No not use ZIP or RAR**. For example, your submission should look like this: `123456789.ipynb` if you worked by yourself or `123456789_987654321.ipynb` if you worked in pairs.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Practice OOP in python.\n",
    "2. Implement two impurity measures: Gini and Entropy.\n",
    "3. Construct a decision tree algorithm.\n",
    "4. Prune the tree to achieve better results.\n",
    "5. Visualize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed9fe7b1026e33cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6ac605270c2b091",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Warmup - OOP in python\n",
    "\n",
    "Our desicion tree will be implemented using a dedicated python class. Take a minute and practice your object oriented skills. Create a tree with some nodes and make sure you understand how objects in python work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<__main__.Node at 0x1b863051340>, <__main__.Node at 0x1b863051730>]"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Node(5)\n",
    "p = Node(6)\n",
    "q = Node(7)\n",
    "n.add_child(p)\n",
    "n.add_child(q)\n",
    "n.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f1ceb251c649b62",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We will use the breast cancer dataset that is available as a part of sklearn. In this example, our dataset will be a single matrix with the **labels on the last column**. Notice that you are not allowed to use additional functions from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79cb4542926ad3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape:  (426, 31)\n",
      "Testing dataset shape:  (143, 31)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset\n",
    "X, y = datasets.load_breast_cancer(return_X_y = True)\n",
    "X = np.column_stack([X,y]) # the last column holds the labels\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test = train_test_split(X, random_state=99)\n",
    "\n",
    "print(\"Training dataset shape: \", X_train.shape)\n",
    "print(\"Testing dataset shape: \", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168 258]\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lables = X_train[:,-1:]\n",
    "l, lable_count = np.unique(lables, return_counts= True)\n",
    "print(lable_count)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd7b0191f3f1e897",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Impurity Measures (10 points)\n",
    "\n",
    "Impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Implement the functions `calc_gini` (5 points) and `calc_entropy` (5 points). You are encouraged to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gini(data):\n",
    "    \"\"\"\n",
    "    Calculate gini impurity measure of a dataset.\n",
    " \n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    " \n",
    "    Returns the gini impurity.    \n",
    "    \"\"\"\n",
    "    gini = 1\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    lables = data[:,-1:]\n",
    "    _ , lable_count = np.unique(lables, return_counts= True)\n",
    "    sum_count = sum(lable_count)\n",
    "    for count in lable_count:\n",
    "        gini -= (count/sum_count)**2\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "\n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    "\n",
    "    Returns the entropy of the dataset.    \n",
    "    \"\"\"\n",
    "    entropy = 0.0\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    lables = data[:,-1:]\n",
    "    _ , lable_count = np.unique(lables, return_counts= True)\n",
    "    sum_count = sum(lable_count)\n",
    "    for count in lable_count:\n",
    "        pj = count/sum_count\n",
    "        entropy -= pj*np.log(pj)  \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_gini(np.array([[0,0],[0,0]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        # you should take more arguments as inputs when initiating a new node\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "\n",
    "        self.feature = None\n",
    "        self.splitting_value = None\n",
    "        unique_vals, counts = np.unique(self.data[:, -1], return_counts=True)\n",
    "        self.prediction = unique_vals[np.argmax(counts)]\n",
    "\n",
    "        self.leaf = True\n",
    "        self.height = 1\n",
    "\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "\n",
    "    def add_child(self, node, side):\n",
    "\n",
    "        self.children.append(node)\n",
    "\n",
    "        if side == 'left':\n",
    "            self.left_child = node\n",
    "        else:\n",
    "            self.right_child = node\n",
    "\n",
    "        if (self.leaf):\n",
    "            self.leaf = False\n",
    "        self.height += node.height\n",
    "\n",
    "\n",
    "    def check_split(self, feature, value):\n",
    "        # this function divides the data according to a specific feature and value\n",
    "        # you should use this function while testing for the optimal split\n",
    "        group_A = self.data[self.data[:,feature] <= value]\n",
    "        group_B = self.data[self.data[:,feature] > value]\n",
    "        return group_A, group_B\n",
    "\n",
    "    def infogain(self, group_A, group_B, impurity_measure):\n",
    "        s=self.data.shape[0]\n",
    "        return impurity_measure(self.data) - ((group_A.shape[0]/s)*impurity_measure(group_A) + (group_B.shape[0]/s)*impurity_measure(group_B))\n",
    "\n",
    "    def split(self, impurity_measure):\n",
    "        # this function goes over all possible features and values and finds\n",
    "        # the optimal split according to the impurity measure. Note: you can\n",
    "        # send a function as an argument\n",
    "        num_of_features = len(self.data[0]) - 1\n",
    "        all_splits_infogains = dict()\n",
    "        for feature in range(num_of_features):\n",
    "            sorted_data_by_feature = np.sort(self.data[:,feature])\n",
    "            thresholds = [(sorted_data_by_feature[i] + sorted_data_by_feature[i + 1]) / 2\n",
    "                  for i in range(len(sorted_data_by_feature) - 1)]\n",
    "\n",
    "            # for i in range(len(sorted_data_by_feature) - 1):\n",
    "            #     thresholds.append(sorted_data_by_feature[i] + sorted_data_by_feature[i+1])/2\n",
    "            for threshold in thresholds:\n",
    "                group_A, group_B = self.check_split(feature, threshold)\n",
    "                all_splits_infogains[self.infogain(group_A, group_B, impurity_measure)] = (feature, threshold)\n",
    "        best_score = max(all_splits_infogains.keys())\n",
    "        best_feature_and_value = all_splits_infogains[best_score]\n",
    "        return best_feature_and_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree (50 points)\n",
    "\n",
    "Use a Python class to construct the decision tree. Your class should support the following functionality:\n",
    "\n",
    "1. Initiating a node for a decision tree. You will need to use several class methods and class attributes and you are free to use them as you see fit. We recommend that every node will hold the **feature** and **value** used for the split and the **children** of that node. In addition, it might be a good idea to store the **prediction** in that node, the **height** of the tree for that node and whether or not that node is a **leaf** in the tree.\n",
    "2. Your code should support both Gini and Entropy as impurity measures. \n",
    "3. The provided data includes continuous data. For this exercise, create at most a **single split** for each node of the tree (your tree will be binary). Determine the threshold for splitting by checking all possible features and the values available for splitting. When considering the values, take the average of each consecutive pair. For example, for the values [1,2,3,4,5] you should test possible splits on the values [1.5, 2.5, 3.5, 4.5]. \n",
    "5. After you complete building the class for a decision node in the tree, complete the function `build_tree`. This function takes as input the training dataset and the impurity measure. Then, it initializes a root for the decision tree and constructs the tree according to the procedure you saw in class.\n",
    "1. Once you are finished, construct two trees: one with Gini as an impurity measure and the other using Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(27, 0.14235)"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = DecisionNode(X_train)\n",
    "node.split(calc_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, impurity, vis = \"\"):\n",
    "    \"\"\"\n",
    "    Build a tree using the given impurity measure and training dataset. \n",
    "    You are required to fully grow the tree until all leaves are pure. \n",
    " \n",
    "    Input:\n",
    "    - data: the training dataset.\n",
    "    - impurity: the chosen impurity measure. Notice that you can send a function\n",
    "                as an argument in python.\n",
    " \n",
    "    Output: the root node of the tree.\n",
    "    \"\"\"\n",
    "    root = DecisionNode(data)\n",
    "    vis = vis + \"       \"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    if impurity(data) == 0:\n",
    "        print(vis +\"---\" +str(root.feature) + \"    prediction: \" + str(root.prediction) + \" Impurity:\" + str(impurity(data)))\n",
    "        return root\n",
    "    else:\n",
    "        root.feature, root.splitting_value = root.split(impurity)\n",
    "        print(vis +\"---\" +str(root.feature) + \"    prediction: \" + str(root.prediction) + \" Impurity:\" + str(impurity(data)))\n",
    "        group_A, group_B = root.check_split(root.feature, root.splitting_value)\n",
    "        root.add_child(build_tree(group_A, impurity, vis), 'left')\n",
    "        root.add_child(build_tree(group_B, impurity, vis), 'right')\n",
    "        return root\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "\n",
    "# def build_tree(data, impurity):\n",
    "#     \"\"\"\n",
    "#     Build a tree using the given impurity measure and training dataset. \n",
    "#     You are required to fully grow the tree until all leaves are pure. \n",
    " \n",
    "#     Input:\n",
    "#     - data: the training dataset.\n",
    "#     - impurity: the chosen impurity measure. Notice that you can send a function\n",
    "#                 as an argument in python.\n",
    " \n",
    "#     Output: the root node of the tree.\n",
    "#     \"\"\"\n",
    "#     root = DecisionNode(data)\n",
    "#     ###########################################################################\n",
    "#     # TODO: Implement the function.                                           #\n",
    "#     ###########################################################################\n",
    "#     if not impurity(data):\n",
    "#         return root\n",
    "#     else:\n",
    "#         root.feature, root.splitting_value = root.split(impurity)\n",
    "#         group_A, group_B = root.check_split(root.feature, root.splitting_value)\n",
    "#         root.add_child(build_tree(group_A, impurity))\n",
    "#         root.add_child(build_tree(group_B, impurity))\n",
    "#         return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ---27    prediction: 1.0 Impurity:0.47768299940488007\n",
      "                  ---3    prediction: 1.0 Impurity:0.14918097520458606\n",
      "                         ---27    prediction: 1.0 Impurity:0.04721644120707591\n",
      "                                ---13    prediction: 1.0 Impurity:0.01673520981324228\n",
      "                                       ---21    prediction: 1.0 Impurity:0.008510482869457103\n",
      "                                              ---None    prediction: 1.0 Impurity:0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# python support passing a function as arguments to another function.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m tree_gini \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimpurity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcalc_gini\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvis\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m    \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m________________________________________\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m tree_entropy \u001B[38;5;241m=\u001B[39m build_tree(data\u001B[38;5;241m=\u001B[39mX_train, impurity\u001B[38;5;241m=\u001B[39mcalc_entropy)\n",
      "Cell \u001B[1;32mIn[81], line 25\u001B[0m, in \u001B[0;36mbuild_tree\u001B[1;34m(data, impurity, vis)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(vis \u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m---\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(root\u001B[38;5;241m.\u001B[39mfeature) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    prediction: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(root\u001B[38;5;241m.\u001B[39mprediction) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Impurity:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(impurity(data)))\n\u001B[0;32m     24\u001B[0m group_A, group_B \u001B[38;5;241m=\u001B[39m root\u001B[38;5;241m.\u001B[39mcheck_split(root\u001B[38;5;241m.\u001B[39mfeature, root\u001B[38;5;241m.\u001B[39msplitting_value)\n\u001B[1;32m---> 25\u001B[0m root\u001B[38;5;241m.\u001B[39madd_child(\u001B[43mbuild_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup_A\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimpurity\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvis\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     26\u001B[0m root\u001B[38;5;241m.\u001B[39madd_child(build_tree(group_B, impurity, vis), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m root\n",
      "Cell \u001B[1;32mIn[81], line 25\u001B[0m, in \u001B[0;36mbuild_tree\u001B[1;34m(data, impurity, vis)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(vis \u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m---\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(root\u001B[38;5;241m.\u001B[39mfeature) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    prediction: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(root\u001B[38;5;241m.\u001B[39mprediction) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Impurity:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(impurity(data)))\n\u001B[0;32m     24\u001B[0m group_A, group_B \u001B[38;5;241m=\u001B[39m root\u001B[38;5;241m.\u001B[39mcheck_split(root\u001B[38;5;241m.\u001B[39mfeature, root\u001B[38;5;241m.\u001B[39msplitting_value)\n\u001B[1;32m---> 25\u001B[0m root\u001B[38;5;241m.\u001B[39madd_child(\u001B[43mbuild_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup_A\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimpurity\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvis\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     26\u001B[0m root\u001B[38;5;241m.\u001B[39madd_child(build_tree(group_B, impurity, vis), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m root\n",
      "    \u001B[1;31m[... skipping similar frames: build_tree at line 25 (2 times)]\u001B[0m\n",
      "Cell \u001B[1;32mIn[81], line 25\u001B[0m, in \u001B[0;36mbuild_tree\u001B[1;34m(data, impurity, vis)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(vis \u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m---\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(root\u001B[38;5;241m.\u001B[39mfeature) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    prediction: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(root\u001B[38;5;241m.\u001B[39mprediction) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Impurity:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(impurity(data)))\n\u001B[0;32m     24\u001B[0m group_A, group_B \u001B[38;5;241m=\u001B[39m root\u001B[38;5;241m.\u001B[39mcheck_split(root\u001B[38;5;241m.\u001B[39mfeature, root\u001B[38;5;241m.\u001B[39msplitting_value)\n\u001B[1;32m---> 25\u001B[0m \u001B[43mroot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuild_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup_A\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimpurity\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvis\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mleft\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m root\u001B[38;5;241m.\u001B[39madd_child(build_tree(group_B, impurity, vis), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m root\n",
      "Cell \u001B[1;32mIn[79], line 30\u001B[0m, in \u001B[0;36mDecisionNode.add_child\u001B[1;34m(self, node, side)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleaf):\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleaf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m node\u001B[38;5;241m.\u001B[39mheight\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# python support passing a function as arguments to another function.\n",
    "\n",
    "tree_gini = build_tree(data=X_train, impurity=calc_gini, vis = \"    \") \n",
    "print(\"________________________________________\")\n",
    "tree_entropy = build_tree(data=X_train, impurity=calc_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree evaluation (10 points)\n",
    "\n",
    "Complete the functions `predict` and `calc_accuracy`.\n",
    "\n",
    "After building both trees using the training set (using Gini and Entropy as impurity measures), you should calculate the accuracy on the test set and print the measure that gave you the best test accuracy. For the rest of the exercise, use that impurity measure. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, instance):\n",
    "    \"\"\"\n",
    "    Predict a given instance using the decision tree\n",
    " \n",
    "    Input:\n",
    "    - root: the root of the decision tree.\n",
    "    - instance: an row vector from the dataset. \n",
    " \n",
    "    Output: the prediction of the instance.\n",
    "    \"\"\"\n",
    "    # pred = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    if node.leaf:\n",
    "        return node.prediction\n",
    "    if (instance[node.feature] <= node.splitting_value):\n",
    "        return predict(node.left_child, instance)\n",
    "    else:\n",
    "        return predict(node.right_child, instance)\n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    # return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "print (X_train[i,-1], predict(tree_gini, X_train[i,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(node, dataset):\n",
    "    \"\"\"\n",
    "    Predict a given dataset using the decision tree\n",
    " \n",
    "    Input:\n",
    "    - node: a node in the decision tree.\n",
    "    - dataset: the dataset on which the accuracy is evaluated\n",
    " \n",
    "    Output: the accuracy of the decision tree on the given dataset (%).\n",
    "    \"\"\"\n",
    "    rights = 0\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    for instace in dataset:\n",
    "        pred = predict(node,instace[:-1])\n",
    "        if pred == int(instace[-1]):\n",
    "            rights += 1\n",
    "    accuracy = 100*(rights/len(dataset))\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(tree_entropy, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the tree (10 points)\n",
    "\n",
    "Complete the function `print_tree`. Your code should do something like this (10 points):\n",
    "```\n",
    "[X0 <= 1],\n",
    "  [X1 <= 2]\n",
    "    [X2 <= 3], \n",
    "       leaf: [{1.0: 10}]\n",
    "       leaf: [{0.0: 10}]\n",
    "    [X4 <= 5], \n",
    "       leaf: [{1.0: 5}]\n",
    "       leaf: [{0.0: 10}]\n",
    "   leaf: [{1.0: 50}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node: DecisionNode, tabs= \"\"):\n",
    "    \"\"\"\n",
    "    Prints the tree similar to the example above.\n",
    "    As long as the print is clear, any printing scheme will be fine\n",
    "    \n",
    "    Input:\n",
    "    - node: a node in the decision tree.\n",
    " \n",
    "    Output: This function has no return value.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    if node.leaf:\n",
    "        print(f'{tabs}leaf: [{node.prediction}]' )\n",
    "    else:\n",
    "        print( f\"{tabs}[{node.feature} <= {node.splitting_value}],\")\n",
    "    if (node.right_child):\n",
    "        print_tree(node.left_child, tabs + \"  \")\n",
    "        print_tree(node.right_child, tabs + \"  \")\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27 <= 0.14235],\n",
      "  [3 <= 696.25],\n",
      "    [27 <= 0.1349],\n",
      "      [13 <= 48.7],\n",
      "        [21 <= 33.349999999999994],\n",
      "          leaf: [1.0]\n",
      "          [21 <= 33.56],\n",
      "            leaf: [0.0]\n",
      "            leaf: [1.0]\n",
      "        [29 <= 0.07857],\n",
      "          leaf: [0.0]\n",
      "          leaf: [1.0]\n",
      "      [27 <= 0.139],\n",
      "        [25 <= 0.35845000000000005],\n",
      "          leaf: [0.0]\n",
      "          leaf: [1.0]\n",
      "        leaf: [1.0]\n",
      "    [1 <= 19.72],\n",
      "      [26 <= 0.25880000000000003],\n",
      "        leaf: [1.0]\n",
      "        leaf: [0.0]\n",
      "      leaf: [0.0]\n",
      "  [13 <= 21.924999999999997],\n",
      "    [27 <= 0.17885],\n",
      "      [27 <= 0.14379999999999998],\n",
      "        leaf: [0.0]\n",
      "        leaf: [1.0]\n",
      "      leaf: [0.0]\n",
      "    [24 <= 0.096595],\n",
      "      leaf: [1.0]\n",
      "      leaf: [0.0]\n"
     ]
    }
   ],
   "source": [
    "print_tree(tree_entropy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post pruning (20 points)\n",
    "\n",
    "Construct a decision tree and perform post pruning: For each leaf in the tree, calculate the test accuracy of the tree assuming no split occurred on the parent of that leaf and find the best such parent (in the sense that not splitting on that parent results in the best testing accuracy among possible parents). Make that parent into a leaf and repeat this process until you are left with the root. On a single plot, draw the training and testing accuracy as a function of the number of internal nodes in the tree. Explain and visualize the results and print your tree (20 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# TODO: Implement the function.                                           #\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
